{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import os\n",
    "import gym\n",
    "import pickle\n",
    "import argparse\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from tensorboardX import SummaryWriter \n",
    "#from utils.utils import *\n",
    "#from utils.zfilter import *\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "non-default argument follows default argument (<ipython-input-3-0aa871040d81>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-0aa871040d81>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    def __init__(self, num_inputs, num_outputs=100, hidden_size): # 100 dim output, continuous draw here.\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m non-default argument follows default argument\n"
     ]
    }
   ],
   "source": [
    "class Actor(nn.Module):\n",
    "    \"\"\"\n",
    "    \n",
    "    RNN or LSTM, sequence to sequence\n",
    "    \n",
    "    input : state 6x 100 matrix, where 6 tokens, 100 dimensional embeddings\n",
    "    \n",
    "    \n",
    "    actor outputs:\n",
    "    \n",
    "    for each sequence in the output, it is going produce a mu of size 1x100 (embedding shape), and then a\n",
    "    corresponding standard deviation:\n",
    "    \n",
    "    \n",
    "    Ex: embedding = 3:\n",
    "    \n",
    "    \n",
    "    [\"I\",\"love\",\"Rahul\"] = [[.2,.31,.4],[.23,.1,.41],[-.75,-.52,.51]] = state\n",
    "    \n",
    "    \n",
    "    mu, std = actor(state)\n",
    "    \n",
    "    \n",
    "    GET ACTION\n",
    "    mu = [[.21,.29,.45],[.221,.13,.421],[-.7,-.22,.58]]\n",
    "    whatever we designate as action sequence length\n",
    "    \n",
    "    std = [[.01,.01,.01],[.01,.01,.01],[.01,.01,.01]]\n",
    "    action = [[gauss(mean=.21,std=.01),gauss(.29,.01),gauss(.45,.01] ...]]\n",
    "    \n",
    "    TODO: lstm, attention \n",
    "    \n",
    "    basic \n",
    "    \n",
    "    seq -> encoder (lstm, attention, layer norm) -> decoder (lstm, attention, layer norm) -> seq\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, num_inputs, num_outputs=100, hidden_size): # 100 dim output, continuous draw here. \n",
    "        super(Actor, self).__init__()\n",
    "     #   self.RNN1 = nn.RNN(num_inputs, hidden_size)\n",
    "      #  self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "      #  self.fc3 = nn.Linear(hidden_size, num_outputs)\n",
    "        \n",
    "     #   self.fc3.weight.data.mul_(0.1)\n",
    "     #   self.fc3.bias.data.mul_(0.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "       # x = torch.tanh(self.fc1(x))\n",
    "      #  x = torch.tanh(self.fc2(x))\n",
    "     #   mu = self.fc3(x)\n",
    "     #   logstd = torch.zeros_like(mu)\n",
    "        std = torch.exp(logstd)\n",
    "        return mu, std\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    \"\"\"\n",
    "    \n",
    "    seq -> encoder (lstm, attention, layer norm) -> latent -> MLP so that it becomes one value, which is used to estimate\n",
    "    Actor critic shiz\n",
    "    \"\"\"\n",
    "    def __init__(self, num_inputs, hidden_size):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = nn.Linear(num_inputs, hidden_size)\n",
    "\n",
    "        self.fc3.bias.data.mul_(0.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.tanh(self.fc1(x))\n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        v = self.fc3(x)\n",
    "        return v\n",
    "    \n",
    "    \n",
    "#class ActorCritic()\n",
    "\n",
    "    # within forward loop, after encoder, \n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    \"\"\"\n",
    "    \n",
    "    (state , action) -> discrimator -> prob\n",
    "    \n",
    "    \n",
    "    what is prob? probability that this action taken with a particular state was done by an expert aka our real data, \n",
    "    or is what the output of action = actor(state) gave. \n",
    "    \n",
    "    seq2 value \n",
    "    \n",
    "    '\n",
    "    '\n",
    "    \"\"\"\n",
    "    def __init__(self, num_inputs, hidden_size):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.fc1 = nn.LSTM1(num_inputs, hidden_size)\n",
    "        do\n",
    "\n",
    "      #  self.fc3.weight.data.mul_(0.1)\n",
    "     #   self.fc3.bias.data.mul_(0.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.tanh(self.fc1(x))\n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        prob = torch.sigmoid(self.fc3(x))\n",
    "        return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-a10c9da0416a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_entropy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_prob_density\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_discrim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiscrim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscrim_optim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdemonstrations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscrim_update_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip_param\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils'"
     ]
    }
   ],
   "source": [
    "#code for training \n",
    "import torch\n",
    "import numpy as np\n",
    "from utils.utils import get_entropy, log_prob_density\n",
    "\n",
    "def train_discrim(discrim, memory, discrim_optim, demonstrations, discrim_update_num, batch_size, clip_param):\n",
    "    \"\"\"\n",
    "    Training the discriminator. \n",
    "\n",
    "    Use binary cross entropy to classify whether or not a sequence was predicted by the expert (real data) or actor. \n",
    "    \"\"\"\n",
    "    memory = np.array(memory)  # s a r s' tuple\n",
    "    states = np.vstack(memory[:, 0]) \n",
    "    actions = list(memory[:, 1]) #actions taken by actor/policy\n",
    "\n",
    "    states = torch.Tensor(states)\n",
    "    actions = torch.Tensor(actions)\n",
    "        \n",
    "    criterion = torch.nn.BCELoss() # classify\n",
    " \n",
    "    for _ in range(discrim_update_num):\n",
    "        \n",
    "        learner = discrim(torch.cat([states, actions], dim=1)) #pass (s,a) through discriminator\n",
    "        demonstrations = torch.Tensor([states, expert_actions]) # pass (s,a) of expert through discriminator\n",
    "        index = torch.randperm(demonstrations.shape[0])\n",
    "        demonstrations_batch = demonstrations[index,:][:int(batch_size)] # batch of expert samples. Initially this was for the entire dataset, which might be too big. \n",
    "        expert = discrim(demonstrations_batch) #discrimator \"guesses\" whether or not these \n",
    "        # actions came from expert or learner\n",
    "        discrim_loss = criterion(learner, torch.ones((states.shape[0], 1))) + \\\n",
    "                        criterion(expert, torch.zeros((demonstrations_batch.shape[0], 1)))\n",
    "                # discrim loss: predict agent is all wrong, get as close to 0, and predict expert is 1, getting as close to 1 as possible. \n",
    "        discrim_optim.zero_grad() # gan loss, it tries to always get it right. \n",
    "        discrim_loss.backward()\n",
    "        discrim_optim.step()\n",
    "\n",
    "        # take these steps, do it however many times specified. \n",
    "\n",
    "    expert_acc = ((discrim(demonstrations_batch) < 0.5).float()).mean() #how often it realized the fake examples were fake\n",
    "    learner_acc = ((discrim(torch.cat([states, actions], dim=1)) > 0.5).float()).mean() #how often if predicted expert correctly. \n",
    "\n",
    "    return expert_acc, learner_acc # accuracy, it's the same kind, but because imbalanced better to look at separately. \n",
    " \n",
    "\n",
    "def train_actor_critic(actor, critic, memory, actor_optim, critic_optim, actor_critic_update_num, batch_size, clip_param):\n",
    "    \"\"\"\n",
    "    Take a PPO step or two to improve the actor critic model,  using GAE to estimate returns. \n",
    "    \n",
    "    In our case each trajectory it most one step, so the value function will have to do. \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    memory = np.array(memory) \n",
    "    # tuple of a regular old RL problem, but now reward is what the discriminator says. \n",
    "    states = np.vstack(memory[:, 0]) \n",
    "    actions = list(memory[:, 1]) \n",
    "    rewards = list(memory[:, 2])  #IRL Rewards\n",
    "    masks = list(memory[:, 3]) \n",
    "\n",
    "    # compute value of what happened, see if what we can get us better. \n",
    "    old_values = critic(torch.Tensor(states))\n",
    "    #GAE aka estimate of Value + actual return roughtly \n",
    "    returns, advants = get_gae(rewards, masks, old_values, gamma, lamda)\n",
    "    \n",
    "    # pass states through actor, get corresponding actions\n",
    "    mu, std = actor(torch.Tensor(states))\n",
    "    # new mus and stds? \n",
    "    old_policy = log_prob_density(torch.Tensor(actions), mu, std) # sum of log probability\n",
    "    # of old actions\n",
    "\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    n = len(states)\n",
    "    arr = np.arange(n)\n",
    "\n",
    "    for _ in range(actor_critic_update_num):\n",
    "        np.random.shuffle(arr)\n",
    "\n",
    "        for i in range(n // batch_size): \n",
    "            batch_index = arr[batch_size * i : batch_size * (i + 1)]\n",
    "            #batch_index = torch.LongTensor(batch_index)\n",
    "            \n",
    "            inputs = torch.Tensor(states)[batch_index]\n",
    "            actions_samples = torch.Tensor(actions)[batch_index]\n",
    "            returns_samples = returns.unsqueeze(1)[batch_index]\n",
    "            advants_samples = advants.unsqueeze(1)[batch_index]\n",
    "            oldvalue_samples = old_values[batch_index].detach()\n",
    "        \n",
    "        \n",
    "            values = critic(inputs) #\n",
    "            clipped_values = oldvalue_samples + \\\n",
    "                             torch.clamp(values - oldvalue_samples,\n",
    "                                         -clip_param, \n",
    "                                         clip_param)\n",
    "            critic_loss1 = criterion(clipped_values, returns_samples)\n",
    "            critic_loss2 = criterion(values, returns_samples)\n",
    "            critic_loss = torch.max(critic_loss1, critic_loss2).mean()\n",
    "\n",
    "            loss, ratio, entropy = surrogate_loss(actor, advants_samples, inputs,\n",
    "                                         old_policy.detach(), actions_samples,\n",
    "                                         batch_index)\n",
    "            clipped_ratio = torch.clamp(ratio,\n",
    "                                        1.0 - clip_param,\n",
    "                                        1.0 + clip_param)\n",
    "            clipped_loss = clipped_ratio * advants_samples\n",
    "            actor_loss = -torch.min(loss, clipped_loss).mean()\n",
    "            #print(actor_loss,critic_loss,entropy)\n",
    "           # return actor_loss, critic_loss, entropy\n",
    "            loss = actor_loss + 0.5 * critic_loss - 0.001 * entropy #entropy bonus to promote exploration.\n",
    "\n",
    "            actor_optim.zero_grad()\n",
    "            loss.backward()\n",
    "            actor_optim.step()\n",
    "\n",
    "           # critic_optim.zero_grad()\n",
    "           # loss.backward() \n",
    "            critic_optim.step()\n",
    "\n",
    "def get_gae(rewards, masks, values, gamma, lamda):\n",
    "    \"\"\"\n",
    "    How much better a particular action is in a particular state. \n",
    "    \n",
    "    Uses reward of current action + value function of that state-action pair, discount factor gamma, and then lamda to compute. \n",
    "    \"\"\"\n",
    "    rewards = torch.Tensor(rewards)\n",
    "    masks = torch.Tensor(masks)\n",
    "    returns = torch.zeros_like(rewards)\n",
    "    advants = torch.zeros_like(rewards)\n",
    "    \n",
    "    running_returns = 0\n",
    "    previous_value = 0\n",
    "    running_advants = 0\n",
    "\n",
    "    for t in reversed(range(0, len(rewards))): #for LL, only ever one step :-)\n",
    "        running_returns = rewards[t] + (gamma * running_returns * masks[t])\n",
    "        returns[t] = running_returns\n",
    "\n",
    "        running_delta = rewards[t] + (gamma * previous_value * masks[t]) - \\\n",
    "                                        values.data[t]\n",
    "        previous_value = values.data[t]\n",
    "        \n",
    "        running_advants = running_delta + (gamma * lamda * \\\n",
    "                                            running_advants * masks[t])\n",
    "        advants[t] = running_advants\n",
    "\n",
    "    advants = (advants - advants.mean()) / advants.std()\n",
    "    return returns, advants\n",
    "\n",
    "def surrogate_loss(actor, advants, states, old_policy, actions, batch_index):\n",
    "    \"\"\"\n",
    "    The loss for PPO. Re-run through network, recomput policy from states\n",
    "    and see if this surrogate ratio is better. If it is, use as proximal policy update. It's very close to prior policy, but def better. \n",
    "    \n",
    "    Not sure this actually works though. Should not the new mu and stds be used to draw,\n",
    "    \n",
    "        When do we use get_action? Only once in main, I think it should be for all? \n",
    "    \"\"\"\n",
    "    mu, std = actor(states)\n",
    "    new_policy = log_prob_density(actions, mu, std)\n",
    "    old_policy = old_policy[batch_index]\n",
    "\n",
    "    ratio = torch.exp(new_policy - old_policy)\n",
    "    surrogate_loss = ratio * advants\n",
    "    entropy = get_entropy(mu, std)\n",
    "\n",
    "    return surrogate_loss, ratio, entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the hyperparameters. It's going to be a bitch, but we need to find a way to optimize these. \n",
    "\n",
    "Some heuristics might help. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normally args but not here :-)\n",
    "env_name = # Dialog\n",
    "load_model = None\n",
    "train_discrim_flag = True # starts here. \n",
    "\n",
    "\n",
    "seed = 0 # should try 3\n",
    "hidden_size = 128 # should try a few, also add more layers? \n",
    "learning_rate = 3e-4 # try a few\n",
    "clip_param = .2 # try a few\n",
    "discrim_update_num = 2 # try a few\n",
    "actor_critic_update_num = 10 # try a few\n",
    "l2_rate = 1e-3 # weight decay # try a few\n",
    "total_sample_size = 256 # total num of state-actions to collect before learning \n",
    "batch_size = 32\n",
    "\n",
    "# That's about 9 values. 9^3 is 729, and a shit load more than we're able to try\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "render = False #X X\n",
    "gamma = 0.99 # no longer used ! \n",
    "lamda = .98 # lambda is used for future steps in GAE. Also only one \n",
    "\n",
    "\n",
    "# Not sure, maybe go for iters\n",
    "suspend_accu_exp = 1. # do not need to be this high typically, but seems likely it has to be for a simple env like mountain car cont.\n",
    "suspend_accu_gen = 1.\n",
    "max_iter_num = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one last setup before main\n",
    "logdir = 'logs'\n",
    "\n",
    "env = gym.make(env_name)\n",
    "    \n",
    "env.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "num_inputs = env.observation_space.shape[0]\n",
    "num_actions = env.action_space.shape[0]\n",
    "running_state = ZFilter((num_inputs,), clip=5) # huh? \n",
    "# oh wow. ZFilter is exactly what I do in capstone project, removing \"badtimes\"\n",
    "\n",
    "print('state size:', num_inputs) \n",
    "print('action size:', num_actions)\n",
    "\n",
    "#load agent stuff \n",
    "actor = Actor(num_inputs, num_actions, hidden_size)\n",
    "critic = Critic(num_inputs, hidden_size)\n",
    "discrim = Discriminator(num_inputs + num_actions, hidden_size)\n",
    "\n",
    "actor_optim = optim.Adam(actor.parameters(), lr=learning_rate)\n",
    "critic_optim = optim.Adam(critic.parameters(), lr=learning_rate, \n",
    "                          weight_decay=l2_rate) \n",
    "discrim_optim = optim.Adam(discrim.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load demonstrations\n",
    "#expert_demo, _ = pickle.load(open('./expert_demo/expert_demo.p', \"rb\"))\n",
    "#demonstrations = np.load('/Users/noahkasmanoff/Desktop/Projects/lets-do-irl/mountaincar/app/expert_demo/expert_demo.npy')\n",
    "print(\"demonstrations.shape\", demonstrations.shape)\n",
    "\n",
    "writer = SummaryWriter(logdir)\n",
    "\n",
    "#if you aren't starting from scratch, load in this \n",
    "if load_model is not None:\n",
    "    saved_ckpt_path = os.path.join(os.getcwd(), 'save_model', str(load_model))\n",
    "    ckpt = torch.load(saved_ckpt_path)\n",
    "\n",
    "    # initialize everything\n",
    "    actor.load_state_dict(ckpt['actor'])\n",
    "    critic.load_state_dict(ckpt['critic'])\n",
    "    discrim.load_state_dict(ckpt['discrim'])\n",
    "\n",
    "    running_state.rs.n = ckpt['z_filter_n']\n",
    "    running_state.rs.mean = ckpt['z_filter_m']\n",
    "    running_state.rs.sum_square = ckpt['z_filter_s']\n",
    "\n",
    "    print(\"Loaded OK ex. Zfilter N {}\".format(running_state.rs.n))\n",
    "\n",
    "# if no old model no worries, start training. \n",
    "episodes = 0\n",
    "for iter in range(max_iter_num):\n",
    "    # for i total trajectories \n",
    "    actor.eval(), critic.eval()\n",
    "    memory = deque()\n",
    "\n",
    "    steps = 0\n",
    "    scores = []\n",
    "\n",
    "    while steps < total_sample_size: \n",
    "        # sample trajectories  (batch size)\n",
    "        state, expert_action = env.reset() #Potential TODO: expert demonstrations saved somewhere else? \n",
    "        score = 0\n",
    "\n",
    "        # TODO: Do we need this anymore? \n",
    "        state = running_state(state) #uh.. again ZFilter related, cleans the state  \n",
    "        \n",
    "        epsteps = 0\n",
    "        done = False\n",
    "        while not done: \n",
    "            epsteps += 1\n",
    "            #run through environment\n",
    "            if render: \n",
    "                env.render()\n",
    "\n",
    "            steps += 1\n",
    "\n",
    "            #TODO: forward pass through RNN\n",
    "            mu, std = actor(torch.Tensor(state).unsqueeze(0)) #pass state through actor network\n",
    "            action = get_action(mu, std)[0] #compute random action\n",
    "             done = env.step(action) #take a step\n",
    "            irl_reward = get_reward(discrim, state, action) #infer what the reward of this action is based on discriminator's get reward \n",
    "\n",
    "            if done:\n",
    "                mask = 0\n",
    "            else:\n",
    "                mask = 1 #if done, save this, \n",
    "\n",
    "            memory.append([state, action, irl_reward, mask])\n",
    "            \n",
    "            #TODO: no longer exists :-)\n",
    "            #next_state = running_state(next_state) #save cleaned next state\n",
    "            #state = next_state #and set to current state, \n",
    "\n",
    "            score += irl_reward #add total reward\n",
    "           # print(\"IRL Reward=\",irl_reward)\n",
    "            if done:\n",
    "                break\n",
    "            #actual sampling done here \n",
    "\n",
    "\n",
    "        \n",
    "        episodes += 1\n",
    "        scores.append(score)\n",
    "        print(\"steps in ep:\", epsteps)\n",
    "    score_avg = np.mean(scores) #how this model did, \n",
    "    print('{}:: {} episode score is {:.2f}'.format(iter, episodes, score_avg))\n",
    "    writer.add_scalar('log/score', float(score_avg), iter) #logg\n",
    "\n",
    "    actor.train(), critic.train(), discrim.train() #now train \n",
    "    if train_discrim_flag: #if this batch optimizes discrim/reward, \n",
    "        # for training the discriminator, classify where state-action pair came from. \n",
    "        expert_acc, learner_acc = train_discrim(discrim, memory, discrim_optim, demonstrations, discrim_update_num, batch_size, clip_param) # see comments in train_model. \n",
    "        print(\"Expert: %.2f%% | Learner: %.2f%%\" % (expert_acc * 100, learner_acc * 100))\n",
    "        if expert_acc > suspend_accu_exp and learner_acc > suspend_accu_gen:\n",
    "            print(\"Now it will only train the policy, seeing as it is good enough at finding the differences between learner and expert trajectories.\")\n",
    "            train_discrim_flag = False #now train only policy, vanilla RL\n",
    "            render = True\n",
    "    #for training actor critic \n",
    "    \n",
    "    # PPO operation, \n",
    "    train_actor_critic(actor, critic, memory, actor_optim, critic_optim, actor_critic_update_num, batch_size, clip_param) # no output, see comments in train_model \n",
    "\n",
    "    if iter % 100:\n",
    "        score_avg = int(score_avg)\n",
    "\n",
    "        model_path = os.path.join(os.getcwd(),'save_model')\n",
    "        if not os.path.isdir(model_path):\n",
    "            os.makedirs(model_path)\n",
    "\n",
    "        ckpt_path = os.path.join(model_path, 'ckpt_'+ str(score_avg)+'.pth.tar')\n",
    "\n",
    "        save_checkpoint({\n",
    "            'actor': actor.state_dict(),\n",
    "            'critic': critic.state_dict(),\n",
    "            'discrim': discrim.state_dict(),\n",
    "            'z_filter_n':running_state.rs.n,\n",
    "            'z_filter_m': running_state.rs.mean,\n",
    "            'z_filter_s': running_state.rs.sum_square,\n",
    "           # 'args': args,\n",
    "            'score': score_avg\n",
    "        }, filename=ckpt_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
