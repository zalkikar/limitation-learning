# -*- coding: utf-8 -*-
import pickle
import spacy
from tensor_helpers import np_to_var
import time
import torch
import numpy as np


def create_states(turn_limit=3):
    """We use a state function to compress the dialogue context and the words already generated in the current utterance
    to an intermediate representation, which will be regarded as the current state.
    For example, s0 = f(p) represents the state at time step 0 and it takes the dialogue context p as input.
    State st is given as st = f(p, a1, a2, . . . , at−1).
    For now, we limit the range of dialogue context to the utterances to 2 conversation turns."""

    dialog_sets = {}
    set_ind = 0
    dialog_sets[set_ind] = []
    with open('./dat/preprocess/formatted_movie_lines.txt', 'r', encoding='utf-8') as out:
        for line in out:
            line = line.replace("\n", "")
            dialog_sets[set_ind].append(
                line.replace(
                    "</s>",
                    "").replace(
                    "</d>",
                    ""))
            if line.split(' ')[-1] == "</d>":
                set_ind += 1
                dialog_sets[set_ind] = []

    state_dict = {}  # state_dict[dialogue_context] = next state
    for dInd, turns in dialog_sets.items():
        if len(turns) == 2:
            state_dict[turns[0]] = turns[1]
        else:
            turns = list(reversed(turns))
            for i in range(len(turns)):
                if i + turn_limit > len(turns):
                    turn_range = len(turns)
                else:
                    turn_range = i + turn_limit
                rolling_turns = ''
                for j in range(i + 1, turn_range):
                    # print((i,j))
                    rolling_turns = turns[j] + " " + rolling_turns
                    state_dict[rolling_turns] = turns[i]
    return state_dict


def getTokVect_fromDoc(doc: spacy.tokens.Doc):
    # document level token vectors (num_tokens, dim_tokens)
    v = []
    for tok in doc:
        if tok.has_vector:
            v.append(tok.vector)
        else:
            continue
    return np.array(v)


# TODO: sentence level token vectors instead of document level (num_sentences, num_tokens, dim_tokens)
# eventually call within function above
# def getTokVect_fromSent(doc: spacy.tokens.Doc):


def create_state_vects(nlp, state_dict):
    state_vects = {}
    for i, (k, v) in enumerate(state_dict.items()):
        state_vects[i] = [
            getTokVect_fromDoc(
                nlp(k)), getTokVect_fromDoc(
                nlp(v))]
    return state_vects


def pad_state_vects(vects, token_padding=False):

    doc_vects = list(vects.values())
    doc_inds = [[i, i] for i in list(vects.keys())]

    # flattened list of matrices from (initial_state,next_state) matrix pairs
    # of document vectors
    doc_vects_flatten = [np.array(v) for subv in doc_vects for v in subv]
    doc_inds_flatten = [i for subi in doc_inds for i in subi]
    assert len(doc_vects_flatten) == len(doc_inds_flatten) == 2 * len(vects)

    # Document level Padding
    lengths = np.array([v.shape[0] for v in doc_vects_flatten])
    max_length = max(lengths)
    dim = doc_vects_flatten[0].shape[1]
    data = np.zeros([len(doc_vects_flatten), max_length, dim])
    for i, seq in enumerate(doc_vects_flatten):
        # each vector in doc_vects_flatten is (N, D) where N is num. tokens and
        # D is vector dim (300 for google news vectors)
        seq_length = seq.shape[0]
        data[i, 0:seq_length, :] = seq
    data = np_to_var(data)
    print(data.shape)

    # re-create state dictionary
    padded_state_vects = {i: [] for i in list(vects.keys())}
    for i in range(len(doc_vects_flatten)):
        padded_state_vects[doc_inds_flatten[i]].append(data[i])
    assert len(padded_state_vects) == len(vects)

    return padded_state_vects

    """
    if token_padding: ## NOT DONE, however, shouldn't do anything at the moment since all token vectors are same dimension
        tok_vects_flatten = np.array([np.array(tok_v) for sent_v in vects for tok_v in np.array(sent_v)])
        print(tok_vects_flatten.shape)
    """


if __name__ == "__main__":

    """
    Given an initial state s0 representing the history of previous dialogues, a well-trained dialogue system should reply
        with a reasonable sentence hw0, w1, . . . , wti generated by selecting a specific word at different time steps.
        The length t is automatically decided by the policy network.
        We aim to find the optimal policy π(at|st) that selects the most appropriate word at each time step.
    """

    state_dict = create_states()
    torch.save(state_dict, './dat/preprocess/raw_states.pt')

    """ # example print
    dist_print = 60
    print('='*200)
    for cs in list(state_dict.keys())[-20:-14]:
        sep = '-' * (dist_print - len(cs)) + '>'
        print(f'current state = {cs} {sep} next state = {state_dict[cs]}')
    """

    nlp_pretrained = spacy.load('en_core_web_lg')  # spacy ner

    nlp = spacy.load('./models/spacy-blank-GoogleNews/')
    # for now go with learned tokenization
    nlp.tokenizer = nlp_pretrained.tokenizer
    # for now go with learned dependency parsing
    nlp.add_pipe(nlp_pretrained.get_pipe('parser'), name='parser')

    state_vects = create_state_vects(nlp, state_dict)
    assert len(state_dict) == len(state_vects)
    torch.save(state_vects, './dat/preprocess/vectorized_states.pt')

    padded_vects = pad_state_vects(state_vects)
    assert len(state_dict) == len(padded_vects)
    torch.save(padded_vects, './dat/preprocess/padded_vectorized_states.pt')
